{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression in StochTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use BART to model ordinal outcomes with a complementary log-log (cloglog) link function (Alam and Linero (2025)).\n",
    "\n",
    "We begin by loading the requisite libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from stochtree import BARTModel, OutcomeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ordinal BART with Cloglog Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal data refers to outcomes that have a natural ordering but undefined distances between categories. Examples include survey responses (strongly disagree, disagree, neutral, agree, strongly agree), severity ratings (mild, moderate, severe), or educational levels (elementary, high school, college, graduate).\n",
    "\n",
    "The cloglog link function is:\n",
    "$$\\text{cloglog}(p) = \\log(-\\log(1-p))$$\n",
    "\n",
    "In the BART framework with cloglog ordinal regression, we model:\n",
    "$$P(Y = k \\mid Y \\geq k, X = x) = 1 - \\exp\\left(-e^{\\gamma_k + \\lambda(x)}\\right)$$\n",
    "\n",
    "where $\\lambda(x)$ is represented by a stochastic tree ensemble and $\\gamma_k$ are cutpoints for the ordinal categories. This link function is asymmetric and particularly appropriate when the probability of being in higher categories changes rapidly at certain thresholds, making it different from the symmetric probit or logit links commonly used in ordinal regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Simulation\n",
    "\n",
    "We begin by simulating from a dataset with an ordinal outcome with three categories, $y_i \\in \\left\\{1,2,3\\right\\}$ whose probabilities depend on covariates, $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "random_seed = 2026\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Sample size and number of predictors\n",
    "n = 2000\n",
    "p = 5\n",
    "\n",
    "# Design matrix and true lambda function\n",
    "X = rng.standard_normal((n, p))\n",
    "beta = np.ones(p) / np.sqrt(p)\n",
    "true_lambda = X @ beta\n",
    "\n",
    "# Set cutpoints for ordinal categories (3 categories: 1, 2, 3)\n",
    "n_categories = 3\n",
    "gamma_true = np.array([-2.0, 1.0])\n",
    "\n",
    "# True ordinal class probabilities\n",
    "true_probs = np.zeros((n, n_categories))\n",
    "true_probs[:, 0] = 1 - np.exp(-np.exp(gamma_true[0] + true_lambda))\n",
    "for j in range(1, n_categories - 1):\n",
    "    true_probs[:, j] = (\n",
    "        np.exp(-np.exp(gamma_true[j - 1] + true_lambda))\n",
    "        * (1 - np.exp(-np.exp(gamma_true[j] + true_lambda)))\n",
    "    )\n",
    "true_probs[:, n_categories - 1] = 1 - true_probs[:, :-1].sum(axis=1)\n",
    "\n",
    "# Generate ordinal outcomes (1-indexed integers)\n",
    "y = np.array(\n",
    "    [rng.choice(np.arange(1, n_categories + 1), p=true_probs[i]) for i in range(n)],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "# Print outcome distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Outcome distribution:\", dict(zip(unique.astype(int), counts)))\n",
    "\n",
    "# Train-test split\n",
    "sample_inds = np.arange(n)\n",
    "train_inds, test_inds = train_test_split(sample_inds, test_size=0.2, random_state=random_seed)\n",
    "X_train = X[train_inds, :]\n",
    "X_test = X[test_inds, :]\n",
    "y_train = y[train_inds]\n",
    "y_test = y[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "We specify the cloglog link function for modeling an ordinal outcome by setting `outcome_model=OutcomeModel(outcome=\"ordinal\", link=\"cloglog\")` in the `general_params` argument. Since ordinal outcomes are incompatible with the Gaussian global error variance model, we also set `sample_sigma2_global=False`.\n",
    "\n",
    "We also override the default `num_trees` for the mean forest (200) in favor of greater regularization for the ordinal model and set `sample_sigma2_leaf=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model = BARTModel()\n",
    "bart_model.sample(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    num_gfr=0,\n",
    "    num_burnin=1000,\n",
    "    num_mcmc=1000,\n",
    "    general_params={\n",
    "        \"cutpoint_grid_size\": 100,\n",
    "        \"sample_sigma2_global\": False,\n",
    "        \"keep_every\": 1,\n",
    "        \"num_chains\": 1,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"outcome_model\": OutcomeModel(outcome=\"ordinal\", link=\"cloglog\"),\n",
    "    },\n",
    "    mean_forest_params={\"num_trees\": 50, \"sample_sigma2_leaf\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "As with any other BART model in `stochtree`, we can use the `predict` function on our ordinal model. Specifying `scale = \"linear\"` and `terms = \"y_hat\"` will simply return predictions from the estimated $\\lambda(x)$ function, but users can estimate class probabilities via `scale = \"probability\"`, which by default will return an array of dimension (`num_observations`, `num_categories`, `num_samples`), where `num_observations = nrow(X)`, `num_categories` is the number of unique ordinal labels that the outcome takes, and `num_samples` is the number of draws of the model. Specifying `type = \"mean\"` collapses the output to a `num_observations` x `num_categories` matrix, with the average posterior class probability for each observation. Users can also specify `type = \"class\"` for the maximum a posteriori (MAP) class label estimate for each draw of each observation.\n",
    "\n",
    "Below we compute the posterior class probabilities for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_probs_train = bart_model.predict(\n",
    "  X_train, scale=\"probability\", terms=\"y_hat\", type=\"mean\"\n",
    ")\n",
    "est_probs_test = bart_model.predict(\n",
    "  X_test, scale=\"probability\", terms=\"y_hat\", type=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one of the \"cutpoints\" is fixed for identifiability, we plot the posterior distributions of the other two cutpoints and compare them to their true simulated values (blue dotted lines).\n",
    "\n",
    "The cutpoint samples are accessed via `bart_model.cloglog_cutpoint_samples` (shape: `(n_categories - 1, num_samples)`) and are shifted by the per-sample mean of the training predictions to account for the non-identifiable intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1 = bart_model.cloglog_cutpoint_samples[0, :] + bart_model.y_hat_train.mean(axis=0)\n",
    "gamma2 = bart_model.cloglog_cutpoint_samples[1, :] + bart_model.y_hat_train.mean(axis=0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.hist(gamma1, density=True, bins=40)\n",
    "ax1.axvline(gamma_true[0], color=\"blue\", linestyle=\"dotted\", linewidth=2)\n",
    "ax1.set_title(\"Posterior Distribution of Cutpoint 1\")\n",
    "ax1.set_xlabel(\"Cutpoint 1\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "\n",
    "ax2.hist(gamma2, density=True, bins=40)\n",
    "ax2.axvline(gamma_true[1], color=\"blue\", linestyle=\"dotted\", linewidth=2)\n",
    "ax2.set_title(\"Posterior Distribution of Cutpoint 2\")\n",
    "ax2.set_xlabel(\"Cutpoint 2\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compare the true latent \"utility function\" $\\lambda(x)$ to the (mean-shifted) BART forest predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_pred_train = bart_model.y_hat_train.mean(axis=1) - bart_model.y_hat_train.mean()\n",
    "lambda_pred_test = bart_model.y_hat_test.mean(axis=1) - bart_model.y_hat_test.mean()\n",
    "corr_train = np.corrcoef(true_lambda[train_inds], lambda_pred_train)[0, 1]\n",
    "corr_test = np.corrcoef(true_lambda[test_inds], lambda_pred_test)[0, 1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(lambda_pred_train, true_lambda[train_inds], alpha=0.3, s=10)\n",
    "ax1.axline((0, 0), slope=1, color=\"blue\", linewidth=2)\n",
    "ax1.set_title(\"Train Set: Predicted vs Actual\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"Actual\")\n",
    "ax1.text(0.05, 0.95, f\"Correlation: {corr_train:.3f}\", transform=ax1.transAxes,\n",
    "         color=\"red\", verticalalignment=\"top\")\n",
    "\n",
    "ax2.scatter(lambda_pred_test, true_lambda[test_inds], alpha=0.3, s=10)\n",
    "ax2.axline((0, 0), slope=1, color=\"blue\", linewidth=2)\n",
    "ax2.set_title(\"Test Set: Predicted vs Actual\")\n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"Actual\")\n",
    "ax2.text(0.05, 0.95, f\"Correlation: {corr_test:.3f}\", transform=ax2.transAxes,\n",
    "         color=\"red\", verticalalignment=\"top\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the estimated posterior mean class probabilities with the true simulated value for each class on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, n_categories, figsize=(15, 5))\n",
    "for j in range(n_categories):\n",
    "    corr = np.corrcoef(true_probs[train_inds, j], est_probs_train[:, j])[0, 1]\n",
    "    axes[j].scatter(true_probs[train_inds, j], est_probs_train[:, j], alpha=0.3, s=10)\n",
    "    axes[j].axline((0, 0), slope=1, color=\"blue\", linewidth=2)\n",
    "    axes[j].set_title(f\"Training Set: True vs Estimated Probability, Class {j + 1}\")\n",
    "    axes[j].set_xlabel(\"True Class Probability\")\n",
    "    axes[j].set_ylabel(\"Estimated Class Probability\")\n",
    "    axes[j].text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=axes[j].transAxes,\n",
    "                 color=\"red\", verticalalignment=\"top\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the same comparison on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, n_categories, figsize=(15, 5))\n",
    "for j in range(n_categories):\n",
    "    corr = np.corrcoef(true_probs[test_inds, j], est_probs_test[:, j])[0, 1]\n",
    "    axes[j].scatter(true_probs[test_inds, j], est_probs_test[:, j], alpha=0.3, s=10)\n",
    "    axes[j].axline((0, 0), slope=1, color=\"blue\", linewidth=2)\n",
    "    axes[j].set_title(f\"Test Set: True vs Estimated Probability, Class {j + 1}\")\n",
    "    axes[j].set_xlabel(\"True Class Probability\")\n",
    "    axes[j].set_ylabel(\"Estimated Class Probability\")\n",
    "    axes[j].text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=axes[j].transAxes,\n",
    "                 color=\"red\", verticalalignment=\"top\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Alam, Entejar, and Antonio R Linero. 2025. “A Unified Bayesian Nonparametric Framework for Ordinal, Survival, and Density Regression Using the Complementary Log-Log Link.” *arXiv Preprint arXiv:2502.00606*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
